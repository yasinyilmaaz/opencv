{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trex Projesi\n",
    "Model Eğitimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from keras import models\n",
    "from keras import layers\n",
    "# Dense = fully connected layer katmanları\n",
    "# Flatten = düzleştirme katmanı\n",
    "# Conv2D = 2D konvolüsyon katmanı\n",
    "# MaxPooling2D = 2D max pooling katmanı\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "# LabelEncoder = etiketlerin sayısal değerlere dönüştürülmesi\n",
    "# OneHotEncoder = etiketler kerasta eğitilebilir hale getirilir\n",
    "from sklearn.preprocessing import OneHotEncoder # eğitim ve test verilerini ayırmak için\n",
    "# import seaborn as sns # grafik çizmek için\n",
    "\n",
    "# uyarıları kapat\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169\n",
      "Epoch 1/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step - accuracy: 0.4371 - loss: 4.2441\n",
      "Epoch 2/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.3581 - loss: 4.4795\n",
      "Epoch 3/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step - accuracy: 0.4473 - loss: 1.8014\n",
      "Epoch 4/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.4528 - loss: 1.3263\n",
      "Epoch 5/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.5736 - loss: 0.9331\n",
      "Epoch 6/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.5788 - loss: 0.8380\n",
      "Epoch 7/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.5689 - loss: 0.8269\n",
      "Epoch 8/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.6105 - loss: 0.6984\n",
      "Epoch 9/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.7210 - loss: 0.6149\n",
      "Epoch 10/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.8107 - loss: 0.5728\n",
      "Epoch 11/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.8104 - loss: 0.5260\n",
      "Epoch 12/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.8367 - loss: 0.4889\n",
      "Epoch 13/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.8421 - loss: 0.4498\n",
      "Epoch 14/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.8473 - loss: 0.4296\n",
      "Epoch 15/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.9000 - loss: 0.3652\n",
      "Epoch 16/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.9052 - loss: 0.3287\n",
      "Epoch 17/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.9052 - loss: 0.3188\n",
      "Epoch 18/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.9158 - loss: 0.2748\n",
      "Epoch 19/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.9210 - loss: 0.2745\n",
      "Epoch 20/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.9263 - loss: 0.2545\n",
      "Epoch 21/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.9263 - loss: 0.2415\n",
      "Epoch 22/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.9683 - loss: 0.1843\n",
      "Epoch 23/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.9684 - loss: 0.1546\n",
      "Epoch 24/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.9789 - loss: 0.1583\n",
      "Epoch 25/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.9736 - loss: 0.1389\n",
      "Epoch 26/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.9422 - loss: 0.1608\n",
      "Epoch 27/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.9894 - loss: 0.0930\n",
      "Epoch 28/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.9790 - loss: 0.0852\n",
      "Epoch 29/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.9894 - loss: 0.0795\n",
      "Epoch 30/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.9895 - loss: 0.0652\n",
      "Epoch 31/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.9685 - loss: 0.1016\n",
      "Epoch 32/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 1.0000 - loss: 0.0537\n",
      "Epoch 33/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 1.0000 - loss: 0.0340\n",
      "Epoch 34/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 1.0000 - loss: 0.0372\n",
      "Epoch 35/35\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.9895 - loss: 0.0378\n",
      "ss\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0150\n",
      "Eğitim Doğruluğu: % 100.0\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9741 - loss: 0.0884\n",
      "Test Doğruluğu: % 97.67441749572754\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "imgs = glob.glob(os.path.join(os.getcwd(), \"img\",\"img_nihai\",\"*.png\"))\n",
    "\n",
    "width = 125\n",
    "height = 50\n",
    "\n",
    "X = [] #Resimler \n",
    "Y = [] # Etiketler\n",
    "\n",
    "for img in imgs:\n",
    "    filename = os.path.basename(img) #resmin tam yolu ile alır\n",
    "    label = filename.split(\"_\")[0] #Resmin adından etiketini alır\n",
    "    im = np.array(Image.open(img).convert(\"L\").resize((width, height))) # resmi açar (yeniden boyutlandırarak )\n",
    "    im = im/255 # 0 ile 1 arasındaki değerlere çeker\n",
    "    X.append(im)\n",
    "    Y.append(label)\n",
    "\n",
    "\n",
    "\n",
    "X = np.array(X) # resimleri numpy dizisine çevirme\n",
    "X = X.reshape(X.shape[0], width, height, 1) # tüm resimleri aynı boyuta getirilir (x.shape[0] => resimlerin sayısı)\n",
    "# 1 değeri resimlerin siyah beyaz olduğunu belirtir\n",
    "\n",
    "# sns.countplot(Y) # etiketlerin sayısını görselleştirme\n",
    "\n",
    "def onehot_labels(values): # etiketleri onehotencoder ile  dönüştürme\n",
    "    label_encoder= LabelEncoder() \n",
    "    integer_encoded = label_encoder.fit_transform(values) # etiketleri sayısal değerlere dönüştürme\n",
    "    onehot_encoder = OneHotEncoder(sparse_output=False) # sparse=False ile matrisi düzleştirir\n",
    "    print(len(integer_encoded))\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1) # etiketleri yeniden şekillendirme. Normalde (169,) geliyor\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded) # tekrardan bir dönüşüm 0 -> 100 | 1 -> 010 | 2 -> 001\n",
    "    return onehot_encoded\n",
    "\n",
    "Y = onehot_labels(Y)\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.25, random_state=2) # elimizdeki veriyi test train olarak ayırdık\n",
    "# random_state = random bölünme\n",
    "\n",
    "# cnn Model\n",
    "model = models.Sequential() # cnn eğitim tabanı içine layerler eklenir\n",
    "model.add(layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(width, height, 1))) # araştır\n",
    "model.add(layers.Conv2D(64, kernel_size=(3, 3), activation='relu')) # ektrandan input belirtmeye gerek yok\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Dropout(0.25)) #seyreltme\n",
    "model.add(layers.Flatten()) # düzleştirme\n",
    "model.add(layers.Dense(128, activation=\"relu\")) # Gizli Katman\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(3, activation=\"softmax\")) # ouyput layer \n",
    "# softmax ikiden fazla çıktı var ise kullanılır\n",
    "\n",
    "# if  os.path.exists(r\"C:\\Users\\90505\\Desktop\\model_egitimi\\convolutional_neural_networks\\trex_weights.h5\"):\n",
    "#     model.load_weights(r\"C:\\Users\\90505\\Desktop\\model_egitimi\\convolutional_neural_networks\\trex_weights.h5\")\n",
    "#     print(\"weights yüklendi\")\n",
    "\n",
    "if os.path.exists(\"path\"):\n",
    "    model.load_weights(\"sdsdd.h5\")\n",
    "    print(\"weights yüklendi\")\n",
    "\n",
    "\n",
    "    # Hata kontrolü\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=\"Adam\", metrics=[\"accuracy\"]) # modeli derleme\n",
    "# loss = hata fonksiyonu (loss'a göre türev alma işlemi gerçekleşir)\n",
    "# optimizer = modelin güncellenme şekli\n",
    "# metrics = modelin başarısını ölçme\n",
    "model.fit(train_X,train_Y, epochs=35, batch_size=64) # training işlemi\n",
    "# epoch = modellerin kaç kez eğiteleceği \n",
    "# batch_size = resimlerin kaça bölünüp eğiteleceği\n",
    "# Mesela 169 resim var elimizde 64'er bulumscsk şejilde gruplara ayrılacak kalanlarda bir grup daha olacak\n",
    "\n",
    "score_train = model.evaluate(train_X, train_Y)\n",
    "# 0. indexsi kaybı verir\n",
    "print(\"Eğitim Doğruluğu: %\", score_train[1]*100)\n",
    "\n",
    "score_test = model.evaluate(test_X, test_Y)\n",
    "print(\"Test Doğruluğu: %\", score_test[1]*100)\n",
    "\n",
    "open(os.path.join(os.getcwd(), \"models.json\"), \"w\").write(model.to_json())\n",
    "model.save_weights(os.path.join(os.getcwd(), \"trex_weight_new.weightss.h5\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
